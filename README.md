In this project, I utilized the LLaMA 2 model and applied NF4 quantization to enhance efficiency. For fine-tuning, I implemented Parameter Efficient Fine-Tuning (PFET) techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA). This  approach significantly optimized the model's performance, enabling it to adapt effectively to specific tasks while reducing resource consumption.
